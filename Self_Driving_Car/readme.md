### Actions : 
Actions are changes to an agents configuration
### Observations:
Observations are what an agent can measure of its environment
### Rewards:

Rewards are used to infer optimal actions to correspond with observation states.

Concepts:

s-State 

a-Action

R-Reward

<img src="https://render.githubusercontent.com/render/math?math=\Gamma"> - Discount

### The Bellman Equation

$V(s)=max_{a} ( {R(s,a)} + {\gamma V(s')})$


% <img src="https://render.githubusercontent.com/render/math?math={V(s)=max_{a} ( {R(s,a)}  {\gamma V(s')})}"> 
